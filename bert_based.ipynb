{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGtX4ph9sb49"
   },
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install transformers torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHnPVC0Isb5B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 77\n",
    "\n",
    "# ensure reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXhX7dQFsb5E"
   },
   "outputs": [],
   "source": [
    "# check if we have GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Model Name and Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'bert-base-uncased'\n",
    "# pretrained_model = 'xlnet-base-cased'\n",
    "# pretrained_model = 'roberta-base'\n",
    "# pretrained_model = 'albert-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6MKkCApsb5G"
   },
   "outputs": [],
   "source": [
    "# if using colab\n",
    "# data_dir = '/content/drive/My Dbrive/EPFL/Machine Learning/ML_course/projects/project2/project_text_classification/Datasets/twitter-datasets'\n",
    "\n",
    "# maybe you'll need to change this\n",
    "data_dir = 'Datasets/twitter-datasets'\n",
    "\n",
    "model_dir = os.path.join('seq', pretrained_model, 'model')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "train_pos_dir = os.path.join(data_dir, 'train_pos_full.txt')\n",
    "train_neg_dir = os.path.join(data_dir, 'train_neg_full.txt')\n",
    "test_data_dir = os.path.join(data_dir, 'test_data.txt')\n",
    "sample_submission_dir = os.path.join(data_dir, 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Model (**CHANGE THE IMPORTED MODEL HERE**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xSsJx5d5sb5d"
   },
   "outputs": [],
   "source": [
    "# get the model we want\n",
    "if pretrained_model == 'bert-base-uncased':\n",
    "    from transformers import BertForSequenceClassification as SequenceClassificationModel\n",
    "elif pretrained_model == 'xlnet-base-cased':\n",
    "    from transformers import XLNetForSequenceClassification as SequenceClassificationModel\n",
    "elif pretrained_model == 'roberta-base':\n",
    "    from transformers import RobertaForSequenceClassification as SequenceClassificationModel\n",
    "elif pretrained_model == 'albert-base-v2'\n",
    "    from transformers import AlbertForSequenceClassification as SequenceClassificationModel\n",
    "\n",
    "model = SequenceClassificationModel.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bw11KW2xsb5f"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"count total trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zptGtWGcsb5H"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_SkYY87sb5J"
   },
   "outputs": [],
   "source": [
    "# check vocabulary size\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIVPrjDksb5L"
   },
   "outputs": [],
   "source": [
    "# these are all the special tokens\n",
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OT5_MOVisb5M"
   },
   "outputs": [],
   "source": [
    "# check max length of the model input\n",
    "max_input_length = tokenizer.max_model_input_sizes[pretrained_model]\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaDYPvtTsb5P"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    \"\"\"tokenize the sentence and cut it if it's too long\"\"\"\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    # - 2 is for cls and sep tokens\n",
    "    tokens = tokens[:max_input_length - 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDzKYVV2sb5R"
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "# Field handles the conversion to Tensor (tokenizing)\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  use_vocab=False,\n",
    "                  tokenize=tokenize_and_cut,\n",
    "                  preprocessing=tokenizer.convert_tokens_to_ids,\n",
    "                  init_token=init_token_idx,\n",
    "                  eos_token=eos_token_idx,\n",
    "                  pad_token=pad_token_idx,\n",
    "                  unk_token=unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype=torch.long, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-5gEUDOsb5S"
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "with open(train_pos_dir) as f:\n",
    "    pos_lines = [line.rstrip('\\n') for line in f]\n",
    "with open(train_neg_dir) as f:\n",
    "    neg_lines = [line.rstrip('\\n') for line in f]\n",
    "with open(test_data_dir) as f:\n",
    "    test_lines = [line.rstrip('\\n')[line.rstrip('\\n').find(',') + 1:] for line in f]\n",
    "    \n",
    "# load data into dataframe\n",
    "pos_df = pd.DataFrame(pos_lines, columns=['text'])\n",
    "pos_df['label'] = 1\n",
    "neg_df = pd.DataFrame(neg_lines, columns=['text'])\n",
    "neg_df['label'] = 0\n",
    "test_df = pd.DataFrame(test_lines, columns=['text'])\n",
    "# because the model input required some label\n",
    "# we won't use this though\n",
    "test_df['label'] = 1\n",
    "\n",
    "df = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkVIx2Mgsb5U"
   },
   "outputs": [],
   "source": [
    "class DataFrameDataset(data.Dataset):\n",
    "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        examples = []\n",
    "        for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            label = row.label\n",
    "            text = row.text\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, train_df, val_df=None, test_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "        if train_df is not None:\n",
    "            train_data = cls(train_df.copy(), text_field, label_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            val_data = cls(val_df.copy(), text_field, label_field, **kwargs)\n",
    "        if test_df is not None:\n",
    "            test_data = cls(test_df.copy(), text_field, label_field, is_test=True, **kwargs)\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ge0VKpMYsb5W"
   },
   "outputs": [],
   "source": [
    "train_size = df.shape[0]\n",
    "val_per = 0.05\n",
    "val_size = int(val_per * train_size)\n",
    "# transform DataFrame into torchtext Dataset\n",
    "train_data, valid_data, test_data = DataFrameDataset.splits(\n",
    "text_field=TEXT, label_field=LABEL, train_df=df[:-val_size], val_df=df[-val_size:], test_df=test_df)\n",
    "\n",
    "# use the following two lines for small scale testing\n",
    "# train_data, valid_data, test_data = DataFrameDataset.splits(\n",
    "# text_field=TEXT, label_field=LABEL, train_df=df[:100], val_df=df[100:200], test_df=test_df[:100])\n",
    "\n",
    "# use the following two lines only for final testing\n",
    "# train_data, valid_data, test_data = DataFrameDataset.splits(\n",
    "# text_field=TEXT, label_field=LABEL, train_df=df[:1], val_df=df[1:2], test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L0SJoA8usb5Y"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of validation examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRQo0O1Jsb5b"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# get gpu if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# get torchtext Iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIo_JDtUsb5h"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfJUe4uMsb5j"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"returns accuracy per batch\"\"\"\n",
    "\n",
    "    # round predictions to the closest integer\n",
    "    softmax = torch.softmax(preds, dim=1)\n",
    "    final_preds = torch.max(softmax, 1, keepdim=True)[1].squeeze(1)\n",
    "    # convert into float for division \n",
    "    correct = (final_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMFvIf6fsb5l"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer):\n",
    "    \"\"\"training procedure\"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    batch_ctn = 1\n",
    "    pbar = tqdm(iterator)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = model(batch.text, labels=batch.label)[:2]\n",
    "        acc = binary_accuracy(logits, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        pbar.set_description(f'loss: {epoch_loss / batch_ctn:.3f} | accu: {epoch_acc / batch_ctn * 100:.2f}%')\n",
    "        batch_ctn += 1\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djm6Vwxusb5o"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    \"\"\"evaluating procedure (we don't need gradient)\"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    batch_ctn = 1\n",
    "    pbar = tqdm(iterator)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            loss, logits = model(batch.text, labels=batch.label)[:2]\n",
    "            acc = binary_accuracy(logits, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            pbar.set_description(f'loss: {epoch_loss / batch_ctn:.3f} | accu: {epoch_acc / batch_ctn * 100:.2f}%')\n",
    "            batch_ctn += 1\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_Ykfj27sb5t"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    \"\"\"record time of a epoch\"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model to resume training (0: don't resume)\n",
    "resumed_epoch = 0\n",
    "# state = torch.load(model_dir + f'{pretrained_model}-e{resumed_epoch:02}-state.pt')\n",
    "# model.load_state_dict(state['state_dict'])\n",
    "# optimizer.load_state_dict(state['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CeabAdX4sb5u"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "valid_losses = []\n",
    "valid_accues = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    if resumed_epoch:\n",
    "        epoch += (resumed_epoch - 1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_accu = train(model, train_iterator, optimizer)\n",
    "    valid_loss, valid_accu = evaluate(model, valid_iterator)\n",
    "    \n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accues.append(valid_accu)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, f'{pretrained_model}-e{epoch + 1:02}-model.pt'))\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, os.path.join(model_dir, f'{pretrained_model}-e{epoch + 1:02}-state.pt'))\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Accu: {train_accu * 100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Accu: {valid_accu * 100:.2f}%')\n",
    "\n",
    "print()\n",
    "print(f'Best Val. loss epoch: {np.argmin(valid_losses) + 1:02} | Val. loss: {min(valid_losses):.3f}')\n",
    "print(f'Best Val. accu epoch: {np.argmax(valid_accues) + 1:02} | Val. accu: {max(valid_accues) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvoLLMtesb5w"
   },
   "outputs": [],
   "source": [
    "# pick an epoch\n",
    "selected_epoch = 0\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, f'{pretrained_model}-e{selected_epoch:02}-model.pt'), map_location=device))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--1qo_Ntsb5x"
   },
   "outputs": [],
   "source": [
    "def test(model, iterator):\n",
    "    \"\"\"testing procedure (we don't need gradient)\"\"\"\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator):\n",
    "            _, logits = model(batch.text, labels=batch.label)[:2]\n",
    "            softmax = torch.softmax(logits, dim=1)\n",
    "            final_preds = torch.max(softmax, 1, keepdim=True)[1].squeeze(1)\n",
    "            predictions.extend(final_preds.tolist())\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get testing data iterator\n",
    "TEST_BATCH_SIZE = 32\n",
    "test_iterator = data.Iterator(test_data, batch_size=TEST_BATCH_SIZE, device=device, shuffle=False, sort=False, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zpCjxFqsb50"
   },
   "outputs": [],
   "source": [
    "# get predictions of test data\n",
    "predictions = test(model, test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pIOgOTmfsb51"
   },
   "outputs": [],
   "source": [
    "# map predictions to match the original\n",
    "label_map = {0: -1, 1: 1}\n",
    "corrected_predictions = list(map(lambda x: label_map[x], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cwyjelQsb53"
   },
   "outputs": [],
   "source": [
    "# load data into dataframe\n",
    "submission = pd.read_csv(sample_submission_dir)\n",
    "submission.Prediction = corrected_predictions\n",
    "submission.to_csv(os.path.join('seq', pretrained_model, f'{pretrained_model}-e{selected_epoch:02}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sequence_cls.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
